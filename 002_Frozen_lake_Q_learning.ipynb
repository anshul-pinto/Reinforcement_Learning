{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake v0 using Q learning\n",
    "\n",
    "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the environment\n",
    "\n",
    "1. Here we'll create the FrozenLake environment.\n",
    "2. OpenAI Gym is a library composed of many environments that we can use to train our agents.\n",
    "3. In our case we choose to use Frozen Lake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Create the Q-table and initialize it \n",
    "\n",
    "\n",
    "   1. Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
    "   2. OpenAI Gym provides us a way to do that: env.action_space.n and env.observation_space.n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros((state_size,action_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000\n",
    "total_test_episodes = 5\n",
    "max_steps = 99\n",
    "\n",
    "learning_rate = 0.8 \n",
    "# gamma = discounting rate\n",
    "gamma = 0.95\n",
    "\n",
    "'''   Exploration parameters\n",
    "eplsilon =  exploration rate\n",
    "max_epsilon = starting epsilon value\n",
    "min_epsilon = minimum exploaration probablility\n",
    "decay_rate = exponential decay factor\n",
    "'''\n",
    "epsilon = 1.0\n",
    "\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.005\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.5209\n",
      "[[1.74923322e-01 4.15917778e-02 3.21399249e-02 4.15509268e-02]\n",
      " [5.41052296e-03 2.17630960e-02 3.49208317e-03 6.34712236e-02]\n",
      " [1.71590974e-02 1.71671252e-02 1.23352138e-02 6.89743346e-02]\n",
      " [1.10634067e-04 1.99511801e-04 7.05226566e-06 5.08009091e-02]\n",
      " [3.31767651e-01 4.64519251e-02 6.74254496e-04 3.32705961e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.01813160e-01 1.74290825e-04 5.17452937e-03 7.78710733e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.30793373e-02 5.72394990e-02 5.19795727e-02 3.35322326e-01]\n",
      " [1.41067808e-02 6.04978863e-01 5.32693489e-02 1.47231020e-03]\n",
      " [5.11345930e-01 5.64180640e-03 3.15707679e-04 2.07388865e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.07123649e-02 2.85577277e-02 8.46730461e-01 1.16989153e-01]\n",
      " [2.91109730e-01 8.29353108e-01 2.06517509e-01 3.15238983e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    #Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose action (a) in current world state (s)\n",
    "        # first we randomize a number\n",
    "        \n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        # if this is greater that epsilon --> exploitation mode (choose largest Q value)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        # otherwise do a radnom choice\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # take action (a) and observe new state & reward \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # update Q table based on the bellman equation\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * \n",
    "                                                    np.max(q_table[new_state,:]) - q_table[state,action])\n",
    "        \n",
    "        state = new_state\n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "    episode+=1\n",
    "        \n",
    "    #calculate new epsilon\n",
    "    \n",
    "    epsilon = min_epsilon + (max_epsilon -min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 39\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 22\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 65\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 22\n",
      "****************************************************\n",
      "EPISODE  4\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
